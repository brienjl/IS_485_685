{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 12\n",
    "Supervised Learning, hyperparameter tuning, model evaluation, and pipelining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing same model from lecture 11 to tune and evaluate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Type_Flower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   Type_Flower  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "\n",
    "# Load Data\n",
    "iris = datasets.load_iris()\n",
    "print(iris.target_names)\n",
    "\n",
    "# Make a dataframe for easier use later\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Make X a DataFrame\n",
    "df_x = pd.DataFrame(X, columns=iris.feature_names)\n",
    "\n",
    "# Make y a DataFrame\n",
    "df_y = pd.DataFrame(y, columns=['Type_Flower'])\n",
    "\n",
    "# Concatenate them together\n",
    "df = pd.concat([df_x, df_y], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  1 16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.93      0.93      0.93        14\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test Split\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "assert len(X_train) + len(X_test) == len(X)\n",
    "\n",
    "# Initialize call to ML Algorithm of choice\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "# Fit the data\n",
    "knn = knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the data\n",
    "y_prediction = knn.predict(X_test)\n",
    "\n",
    "# Classification Report of predictions\n",
    "print(confusion_matrix(y_test, y_prediction))\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n",
      "Average 5-Fold CV Score: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize call to ML Algorithm of choice\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on Regularization\n",
    "Regularization is highly important, primarily if your model starts to overfit your data. What does that mean? It means that your model starts memorizing your training data and is giving up accuracy on the testing data. A tell-tell sign that you're overfitting is if you training accuracy is 97% and your test accuracy is 65%. This means your model is just memorizing what it saw, rather than learning the important features of the data. This is why regularization exists. It is a parameter that is ADDED to the original model to introduce some \"noise.\" This in return helps the model to not memorize the training data, but start to learn the important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and the ROC curve for BINARY Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Iris dataset binary for an example of ROC curve\n",
    "df_binary = df.loc[(df['Type_Flower'] == 1) | (df['Type_Flower'] == 2)]\n",
    "X = df_binary.iloc[:, :-1]\n",
    "y = df_binary.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.87      0.93        23\n",
      "          2       0.85      1.00      0.92        17\n",
      "\n",
      "avg / total       0.94      0.93      0.93        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on Logistic Regression.\n",
    "Logistic Regression initially gives a probability associated with each prediction based on what category it believes the value should be in. For example, when predicting flower of type 0 we might get a .9765 probability that the model thinks this flower is indeed of type zero. When we run the actual .predict(X_test) method it gives a solid yes or no answer. This is because logistic regression has a decision boundary for grouping a yes or no value. Naturally, the decision is at 0.5. Meaning if a predicted probability is .51 then it'll say its in group 1, and if the predicted probability is .49 then it'll say its in group 2. You can adjust this decision boundary by writing some code on the .predict_proba() method for determining the cut off. You would only want to adjust this decision boundary if your data is highly unbalanced or you gain some indication it should be adjusted through tuning & checking your ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example code that shifts the decision boundary at .3, meaning anything below will be group 1 \n",
    "# and anything above is group 2\n",
    "\n",
    "# y_pred = (clf.predict_proba(X_test)[:,1] >= 0.3).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.59612247  0.87283712  0.86595983  0.17639658  0.30824221  0.30266764\n",
      "  0.58663323  0.85197289  0.24447078  0.09959481  0.50566895  0.17834566\n",
      "  0.66284785  0.75913169  0.90588204  0.27317535  0.61609085  0.63901159\n",
      "  0.14814551  0.12569827  0.93537996  0.62221832  0.23609488  0.16893173\n",
      "  0.80068523  0.09219525  0.42618985  0.79409646  0.33518779  0.9432375\n",
      "  0.22426304  0.15217323  0.86298352  0.35298595  0.91994824  0.39290035\n",
      "  0.78440381  0.84828385  0.25642887  0.45978556]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1022094e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "print(y_pred_prob)\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob, pos_label=2)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr,tpr, label=\"Logistic Regression\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reason we do .predict_proba(X_test) rather than the actual predictions is because we want to see what the model output for predictions prior to establishing a decision based on the models decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing AUC (Area under ROC)\n",
    "AUC is essentially just a single metric that displays the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9872122762148338\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test==2, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with GridSearchCV\n",
    "This allows us to iteratively tune a model over some hyperparameters. The hyperparameter, C, in logistic regression is the inverse of the regularization strength. A large C can lead to an overfit model, while a small C can lead to an underfit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 3.7275937203149381}\n",
      "Best score is 0.97\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining your algorithm steps\n",
    "In a pipeline you can:\n",
    "* Impute missing values\n",
    "* Center & Scale your data\n",
    "* Hyperparameter Tune your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick word on Imputation\n",
    "Imputation is a critical step for filling in missing values. Although not done here, often data in the real world will have missing values. You can impute, or fill in, these missing values with some care and consideration. If you have thousands of records and are only missing 3-5 columns total, you might just through those records out as they won't affect the model's performance. However, if 20% of your data is missing some value then you definitely do not want to throw away 20% of your data as that's valuable information. Instead you can impute these values by taking the mean, median, or mode of that column and fill in the missing values. This imputation is up to the subject matter expert as to which metric is best, or if imputation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for scaling & fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Scaling: 1.0\n",
      "Accuracy without Scaling: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# X & y from above\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=.3, random_state=42)\n",
    " \n",
    "# Fit the pipeline to the training set: knn_scaled\n",
    "knn_scaled = pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Instantiate and fit a k-NN classifier to the unscaled data\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test,y_test)))\n",
    "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for scaling, hyperparameter tuning, and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.78      1.00      0.88         7\n",
      "\n",
      "avg / total       0.95      0.93      0.93        30\n",
      "\n",
      "Tuned Model Parameters: {'SVM__C': 100, 'SVM__gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=21)\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters)\n",
    "# Fit to the training set\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
